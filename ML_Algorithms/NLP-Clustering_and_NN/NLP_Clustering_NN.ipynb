{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP (NLTK brown corpus)- Clustering & Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "import numpy\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Brown corpus (using nltk.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legends: \n",
    "  1. For clarity all the local variable names used in notebook are specified in braces. \n",
    "  2. For description of algorithm and logic flow semantic terms are used (with variable names in braces)\n",
    "  \n",
    "  Purpose : \n",
    "  For NLP or English language based application we are making an endeavour to have a clustering or embedding of these words. Basically similar-semantic meaning words will be grouped close to each other \n",
    "  \n",
    "Input Data:  \n",
    "Loading the Brown-corpus words from the \"NLTK\" library, and using the inbuilt function created a list of all the available words in local list. This list will contain stop words, punctutions etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brwn_lst = brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    }
   ],
   "source": [
    "print (len(brwn_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and punctuation, make everything lowercase, and counting how often each word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extended the inbuilt available \"String Punctuations\" list to include few extra punctions like '--' etc.\n",
    "2.  Removed all the punctuations from list (words_lst)\n",
    "3. Changed all words to lower case in list (words_lst)\n",
    "4. Removed all the \"ENGLISH_STOP_WORDS\" & \"nltk_stops\" - as stop words don't contribute much to the sematic relevance when embedding a given words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extras = ['``','--',\"''\"]\n",
    "punctuations = extras + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013320"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst = [x for x in brwn_lst if x not in punctuations]\n",
    "len(words_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_lst = [x.lower() for x in words_lst ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486942"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst = [x for x in words_lst if x not in text.ENGLISH_STOP_WORDS]\n",
    "len(words_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483756"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stops = stopwords.words('english')\n",
    "words_lst = [x for x in words_lst if x not in nltk_stops ]  #Removing nltk.corpus stopwords\n",
    "len(words_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49474"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_dict = {}\n",
    "for word in words_lst :\n",
    "    word_freq_dict[word] = word_freq_dict.setdefault(word, 0) + 1\n",
    "\n",
    "len(word_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_sorted_lst  = sorted(word_freq_dict.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = [x[0] for x in words_sorted_lst[:5000] ]\n",
    "#print (V[-10:])\n",
    "C = V[:1000]\n",
    "#print (C[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocubulary & Context words list creation \n",
    "\n",
    "Based on the usage/occurance frequency (high usage) of each word created a Vocubulary list (V) of around 5k words and Context list (C) of 1000 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the probability distribution Pr(c|w) of con- text words around w (for each w ∈ V ), as well as the overall distribution Pr(c) of context words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_WC = numpy.zeros(shape=(1000,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AL - Uncommment the following line for adding the smoothing one \n",
    "#N_WC[N_WC == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483756\n",
      "483760\n"
     ]
    }
   ],
   "source": [
    "## AL - Padding list with 2 extra blank strings to addess boundary cases of words. \n",
    "print (len(words_lst))\n",
    "words_lst.insert(0, \"\")\n",
    "words_lst.insert(0, \"\")\n",
    "words_lst.append(\"\")\n",
    "words_lst.append(\"\")\n",
    "print (len(words_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in V:\n",
    "    indices = [i for i, x in enumerate(words_lst) if x == w]\n",
    "    word_idx = V.index(w)\n",
    "    \n",
    "    for idx in range(len(indices)):\n",
    "        word_loc = indices[idx]\n",
    "        w1 = words_lst[word_loc-2]\n",
    "        w2 = words_lst[word_loc-1]\n",
    "        w3 = words_lst[word_loc+1]\n",
    "        w4 = words_lst[word_loc+2]\n",
    "        w =  words_lst[word_loc]\n",
    "\n",
    "        word_set = set([w1, w2, w3, w4])\n",
    "\n",
    "        for elem in word_set:\n",
    "            contxt_idx = -1\n",
    "            try:\n",
    "                contxt_idx = C.index(elem)\n",
    "            except: \n",
    "                continue\n",
    "            else:\n",
    "                N_WC[contxt_idx,word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "[ 3358.  2888.  2867. ...,    31.    44.    24.]\n"
     ]
    }
   ],
   "source": [
    "cols_sum = N_WC.sum(axis=0)\n",
    "print (len(cols_sum))\n",
    "print (cols_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[ 5691.  4888.  4732.  3560.  3534.  2497.  2886.  2656.  2605.  2215.]\n"
     ]
    }
   ],
   "source": [
    "rows_sum = N_WC.sum(axis=1)\n",
    "print (len(rows_sum))\n",
    "print (rows_sum[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568892.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows_sum = np.sum(rows_sum)\n",
    "#total_rows_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Occurance of C in window W\n",
    "2.  Padded the list V with two empty string to handle the boudary cases of four words window.\n",
    "3.  Calculated the n(w, c) = # of times c occurs in a window around w in a Matrix form (numpy array of dim VxC as N_WC)\n",
    "4. Using the count in each cell - construct the probability distribution Pr(c|w) of context words around w, for all words in V (Pr_CW).\n",
    "5.  Also calculated the Probabilties of each word (say c) in C (Pr_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###  Represent each vocabulary item w by a |C|-dimensional vector φ(w), whose c’th coordinate is:\n",
    "φ(w) = max(0, log Pr(c|w) ) Pr(c)\n",
    "#### This is known as the (positive) pointwise mutual information, useful work on word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01000366,  0.00859214,  0.00831792,  0.00625778,  0.00621208,\n",
       "        0.00438923,  0.00507302,  0.00466872,  0.00457908,  0.00389353])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pr_C = rows_sum / total_rows_sum\n",
    "Pr_C[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pr_CW = numpy.zeros(shape=(1000,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(N_WC.shape[0]):\n",
    "    for col in range(N_WC.shape[1]):\n",
    "        Pr_CW[row][col] = N_WC[row][col]/np.sum(cols_sum[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Phi_W = np.log(Pr_CW.T/Pr_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi_W[Phi_W < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi_W[np.isnan(Phi_W)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Using the above information i.e metrices calculated the \"(positive) pointwise mutual information\" by φ(w) = max(0, log Pr(c|w) ) Pr(c)\n",
    "\n",
    "2. We are able to represent each word embedded by context-words window of 4 words in a vector form of dimention 1000\n",
    "3. Using the PCA dimensionality reduction, represented each word in V in high relevance or top  wieghted 100 dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to 100-dimensional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "Phi_W_transformed = pca.fit_transform(Phi_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using KMeans un-supervised learning algorithm to find the 100 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, max_iter=300, random_state=0).fit(Phi_W_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "cluster_1 = []\n",
    "cluster_88 = []\n",
    "for cluster_num in kmeans.labels_:         #[0,0,0,0,1,2,3,100...]\n",
    "    if cluster_num == 1:                 #Use Label as e.g. 90\n",
    "        cluster_1.append(V[j])\n",
    "    elif cluster_num == 88:\n",
    "        cluster_88.append(V[j]) \n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Used the KMeans un-supervised learning algorithm to find the 100 clusters of similar meaning words in Vocubulary (V)\n",
    "2. As a sample print the words belong to specific cluster (e.g. Cluster = 1, 3, 5 & 88)\n",
    "3.  Observed \n",
    "    - That the lenght of each cluster is different i.e. number of words in clusters are different.\n",
    "    - Words in each cluster have similar or associated contextually \n",
    "    - Cluster-1 has words like -  reading, published, journal, newspapers', 'illustrated', 'survey', 'edition' etc. \n",
    "    - Cluster-88 has words like - 'lines', 'points', 'image', 'plane', 'fixed','curve', 'meets', 'pencil','tangent', 'transformed', 'arbitrary', 'transformation', 'curves', 'vertex'\n",
    "4. From the sample it's obvious that dimensionlaity reduction enabled to cluster the vectors without major loss in information. As KMeans created 100 clusters which are meaning.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_1\n",
    "\n",
    "['reading', 'published',\n",
    " 'mentioned', 'experiments', 'newspaper', 'showing', 'ended', 'notes', 'publication', 'ages', 'partly',\\\n",
    " 'mail', 'numerous', 'collected', 'journal' 'newspapers', 'illustrated', 'survey', 'edition', 'schedule',   'visitors','latest','discussions','articles', 'marks', '1953', 'quoted', 'correspondence', 'magazines', 'weekly', 'originally', 'troubles', 'steele', 'supplement']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_3\n",
    "\n",
    "['pay',\n",
    " 'industry',\n",
    " 'market',\n",
    " 'industrial',\n",
    " 'sales',\n",
    " 'farm',\n",
    " 'income',\n",
    " 'products',\n",
    " 'demand',\n",
    " 'share',\n",
    " 'construction',\n",
    " 'companies',\n",
    " 'product',\n",
    " 'capital',\n",
    " 'increases',\n",
    " 'potential',\n",
    " 'substantial',\n",
    " 'employees',\n",
    " 'benefit',\n",
    " 'competition',\n",
    " 'budget',\n",
    " 'housing',\n",
    " 'vehicles',\n",
    " 'raise',\n",
    " 'expense',\n",
    " 'shares',\n",
    " 'expenditures',\n",
    " 'salary',\n",
    " 'investment',\n",
    " 'marketing',\n",
    " 'wages',\n",
    " 'consumer',\n",
    " 'substantially',\n",
    " 'producing',\n",
    " 'financing',\n",
    " 'household',\n",
    " 'retail',\n",
    " 'earnings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_5\n",
    "\n",
    "['company',\n",
    " 'equipment',\n",
    " 'food',\n",
    " 'plant',\n",
    " 'radio',\n",
    " 'machine',\n",
    " 'supply',\n",
    " 'techniques',\n",
    " 'materials',\n",
    " 'model',\n",
    " 'shelter',\n",
    " 'electric',\n",
    " 'electronic',\n",
    " 'commercial',\n",
    " 'machinery',\n",
    " 'uses',\n",
    " 'plants',\n",
    " 'critical',\n",
    " 'improved',\n",
    " 'machines',\n",
    " 'foods',\n",
    " 'efficiency',\n",
    " 'advertising',\n",
    " 'manufacturers',\n",
    " 'purchase',\n",
    " 'supplies',\n",
    " 'periods',\n",
    " 'developments',\n",
    " 'expensive',\n",
    " 'transportation',\n",
    " 'storage',\n",
    " \"today's\",\n",
    " 'automatic',\n",
    " 'tool',\n",
    " 'improve',\n",
    " 'handling',\n",
    " 'processing',\n",
    " 'foam',\n",
    " 'supplied',\n",
    " 'stored',\n",
    " 'suitable',\n",
    " 'tools',\n",
    " 'quantity',\n",
    " 'efficient',\n",
    " 'plastic',\n",
    " 'plastics',\n",
    " 'drying',\n",
    " 'surplus',\n",
    " \"company's\",\n",
    " 'sba',\n",
    " 'manufacturing',\n",
    " 'gin',\n",
    " 'manufacturer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_88\n",
    "\n",
    "['af',\n",
    " 'lines',\n",
    " 'points',\n",
    " 'image',\n",
    " 'plane',\n",
    " 'fixed',\n",
    " 'follows',\n",
    " 'p',\n",
    " 'q',\n",
    " 'curve',\n",
    " 'meets',\n",
    " 'pencil',\n",
    " '**zg',\n",
    " 'tangent',\n",
    " 'transformed',\n",
    " 'arbitrary',\n",
    " 'transformation',\n",
    " 'curves',\n",
    " 'vertex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Nearest Neighbours for some test words using 'cosine' distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(algorithm='brute', metric='cosine', n_neighbors=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=2, p=2, radius=1.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(Phi_W_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_words_lst = ['communism', 'autumn', 'cigarette', 'pulmonary', 'mankind', 'africa', \\\n",
    "'chicago', 'september', 'chemical', 'detergent', 'dictionary', 'storm', 'worship', 'afternoon','mount', 'current', \\\n",
    "                 'school', 'married', 'legislators', 'voters','judges', 'million','washington' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  Neighbour(closest)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'africa': 'asia',\n",
       " 'afternoon': 'went',\n",
       " 'autumn': 'summer',\n",
       " 'chemical': 'clinical',\n",
       " 'chicago': 'portland',\n",
       " 'cigarette': 'lighted',\n",
       " 'communism': 'danger',\n",
       " 'current': 'provide',\n",
       " 'detergent': 'indirect',\n",
       " 'dictionary': 'text',\n",
       " 'judges': 'congressional',\n",
       " 'legislators': 'supervision',\n",
       " 'mankind': 'world',\n",
       " 'married': 'marriage',\n",
       " 'million': 'billion',\n",
       " 'mount': 'injured',\n",
       " 'pulmonary': 'artery',\n",
       " 'school': 'schools',\n",
       " 'september': 'july',\n",
       " 'storm': 'noon',\n",
       " 'voters': 'reform',\n",
       " 'washington': 'president',\n",
       " 'worship': 'protestant'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_neigh_dict = {}\n",
    "\n",
    "for w in test_words_lst:\n",
    "    res = neigh.kneighbors([ Phi_W_transformed[V.index(w)] ])\n",
    "    ngh_word = V[ res[1][0][1] ]\n",
    "    word_neigh_dict[w] = ngh_word\n",
    "    \n",
    "print (\"Word\",  \" Neighbour(closest)\")   \n",
    "word_neigh_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word  Neighbour(closest)\n",
    "\n",
    "{'africa': 'asia',\n",
    " 'afternoon': 'went',\n",
    " 'autumn': 'summer',\n",
    " 'chemical': 'clinical',\n",
    " 'chicago': 'portland',\n",
    " 'cigarette': 'lighted',\n",
    " 'communism': 'danger',\n",
    " 'current': 'provide',\n",
    " 'detergent': 'indirect',\n",
    " 'dictionary': 'text',\n",
    " 'judges': 'congressional',\n",
    " 'legislators': 'supervision',\n",
    " 'mankind': 'world',\n",
    " 'married': 'marriage',\n",
    " 'million': 'billion',\n",
    " 'mount': 'injured',\n",
    " 'pulmonary': 'artery',\n",
    " 'school': 'schools',\n",
    " 'september': 'july',\n",
    " 'storm': 'noon',\n",
    " 'voters': 'reform',\n",
    " 'washington': 'president',\n",
    " 'worship': 'protestant'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a final step Used the Nearest neighbor on the Reduce dimensions vectors\n",
    "  - Used the Metric as 'cosine' and calculated the closest default number of neighbour as 2. Note the first closet neighbour to a given vector is the vector itself. \n",
    "  - Used a sample list of words (test_words_lst) to find the fist closet neighbour and printed the resluts. \n",
    "2. Below is the output - obviously this also makes sense (i.e. the word and it's closent semantically meaning similar word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
